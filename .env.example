# ╔══════════════════════════════════════════════════════════════╗
# ║               MCP Factory — Environment Config              ║
# ╚══════════════════════════════════════════════════════════════╝

# LLM Provider: ollama | openai | groq | together | lmstudio | vllm
LLM_PROVIDER=groq

# Model name (varies by provider)
#   Ollama:    llama3.3:70b, qwen2.5-coder:32b, deepseek-coder-v2
#   OpenAI:    gpt-4o, gpt-4o-mini
#   Groq:      llama-3.3-70b-versatile, openai/gpt-oss-120b
#   Together:  meta-llama/Llama-3.3-70B-Instruct-Turbo
LLM_MODEL=openai/gpt-oss-120b

# Base URL for the LLM API
#   Ollama:    http://localhost:11434
#   LM Studio: http://localhost:1234/v1
#   Groq:      https://api.groq.com/openai/v1
#   Together:  https://api.together.xyz/v1
#   OpenAI:    https://api.openai.com/v1
LLM_BASE_URL=https://api.groq.com/openai/v1

# API key (not needed for local Ollama/LM Studio)
LLM_API_KEY=your-api-key-here

# Timeout in seconds
LLM_TIMEOUT=120

# Server
HOST=0.0.0.0
PORT=8000

# Templates
TEMPLATE_MANIFEST_PATH=../templates/TEMPLATE_MANIFEST.json

# Conversation limits
MAX_CLARIFICATION_ROUNDS=5
COMPLETENESS_THRESHOLD=0.85

# CORS
FRONTEND_URL=http://localhost:5173
